# General Pipeline - é€šç”¨æ•°æ®äº§çº¿æ¡†æ¶

ä¸€å¥—ç”Ÿäº§çº§ã€æ ‡å‡†åŒ–çš„æ•°æ®äº§çº¿æ¡†æ¶ï¼Œæ”¯æŒå±‚çº§åŒ–TOMLé…ç½®ã€å¤šäº‘å­˜å‚¨ã€Dockerå®¹å™¨åŒ–éƒ¨ç½²ã€é€‰æ‹©æ€§æ‰§è¡Œå’Œå®æ—¶èµ„æºç›‘æ§ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### æ¶æ„è®¾è®¡
- **ä¸‰å±‚æ¶æ„**ï¼šPipelineï¼ˆäº§çº¿å±‚ï¼‰â†’ Nodeï¼ˆèŠ‚ç‚¹å±‚ï¼‰â†’ Operatorï¼ˆç®—å­å±‚ï¼‰
- **å…³æ³¨ç‚¹åˆ†ç¦»**ï¼šProjectInitiatorï¼ˆåˆå§‹åŒ–ï¼‰+ PipelineExecutorï¼ˆæ‰§è¡Œï¼‰
- **Dockerå‹å¥½**ï¼šæ”¯æŒå¤šé˜¶æ®µæ„å»ºï¼Œåˆå§‹åŒ–ä¸è¿è¡Œåˆ†ç¦»

### é…ç½®ç®¡ç†
- **TOMLæ ¼å¼**ï¼šæ›´å¼ºçš„ç±»å‹å®‰å…¨æ€§ï¼Œæ›´æ¸…æ™°çš„åµŒå¥—ç»“æ„
- **å±‚çº§åŒ–åŠ è½½**ï¼šPipeline/Node/Operatoråˆ†æ–‡ä»¶ç®¡ç†ï¼Œæ”¯æŒç‰ˆæœ¬æ§åˆ¶
- **åŠ¨æ€è¦†ç›–**ï¼šä»S3åŠ è½½é…ç½®è¦†ç›–ï¼Œæ”¯æŒç¯å¢ƒç‰¹å®šé…ç½®
- **PydanticéªŒè¯**ï¼šå¼ºç±»å‹æ ¡éªŒï¼Œæ—©æœŸå‘ç°é…ç½®é”™è¯¯

### è™šæ‹Ÿç¯å¢ƒ
- **å¤šç¯å¢ƒæ”¯æŒ**ï¼šUVï¼ˆpyproject.tomlï¼‰ã€Pixiï¼ˆpixi.tomlï¼‰ã€Condaï¼ˆS3å‹ç¼©åŒ…ï¼‰
- **è‡ªåŠ¨å¤ç”¨**ï¼šç›¸åŒç¯å¢ƒåªåˆ›å»ºä¸€æ¬¡
- **çµæ´»æ¿€æ´»**ï¼š`activate_env_cmd()` è¿”å›å‘½ä»¤åˆ—è¡¨ï¼Œé€‚é…ä¸åŒæ¿€æ´»æ–¹å¼

### å¯¹è±¡å­˜å‚¨
- **å¤šäº‘æ”¯æŒ**ï¼šAWS S3ã€ç«å±±å¼•æ“TOSã€é‡‘å±±äº‘KS3ã€é˜¿é‡Œäº‘OSSã€è…¾è®¯äº‘COS
- **ç»Ÿä¸€æ¥å£**ï¼š`provider://bucket/key` æ ¼å¼ï¼Œè‡ªåŠ¨è·¯ç”±åˆ°æ­£ç¡®æä¾›å•†
- **å®‰å…¨å‡­è¯**ï¼šä» `s3_aksk.env` åŠ è½½ï¼Œä¸æš´éœ²åœ¨ä»£ç ä¸­
- **ä¾¿æ·æ–¹æ³•**ï¼š`download_from_s3()` å’Œ `upload_to_s3()` è‡ªåŠ¨ç®¡ç†å®¢æˆ·ç«¯

### æ‰§è¡Œæ§åˆ¶
- **é€‰æ‹©æ€§æ‰§è¡Œ**ï¼šè¿è¡Œå•ä¸ªç®—å­ã€å•ä¸ªèŠ‚ç‚¹æˆ–å…¨éƒ¨èŠ‚ç‚¹
- **æ‰§è¡Œå±‚æ¬¡**ï¼š`run()` â†’ `run_node()` â†’ `run_op()` æ¸…æ™°åˆ†å±‚
- **å®æ—¶ç›‘æ§**ï¼šCPUã€å†…å­˜ã€ç£ç›˜IOã€ç½‘ç»œIOã€GPUï¼ˆå¯é€‰ï¼‰
- **è¶…æ—¶æ§åˆ¶**ï¼šç®—å­çº§åˆ«è¶…æ—¶è®¾ç½®

### é¡¹ç›®ç®¡ç†
- **æ ¹ç›®å½•æ ‡è®°**ï¼šä½¿ç”¨ `.project_root` æ–‡ä»¶æ ‡è¯†é¡¹ç›®æ ¹
- **è‡ªåŠ¨å…‹éš†**ï¼šä½¿ç”¨GitPythonè‡ªåŠ¨å…‹éš†ç®—å­ä»£ç 
- **é…ç½®å¯¼å‡º**ï¼šé›†æˆé…ç½®è‡ªåŠ¨å¯¼å‡ºåˆ° `conf/integration/` ç”¨äºå®¡è®¡

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

- Python 3.11+
- Linux-amd64 æ“ä½œç³»ç»Ÿ
- Git
- å¯é€‰ï¼šUVã€Pixiã€Condaï¼ˆæ ¹æ®ä½¿ç”¨çš„è™šæ‹Ÿç¯å¢ƒï¼‰
- å¯é€‰ï¼šboto3ï¼ˆå¦‚æœä½¿ç”¨å¯¹è±¡å­˜å‚¨åŠŸèƒ½ï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…

```bash
git clone https://github.com/HernandoR/general_pipeline.git
cd general_pipeline
pip install -e .
```

### 2. è®¾ç½®é¡¹ç›®æ ¹ç›®å½•

```bash
touch .project_root
```

### 3. é…ç½®S3å‡­è¯ï¼ˆå¦‚æœéœ€è¦ï¼‰

```bash
cp s3_aksk.env.example s3_aksk.env
# ç¼–è¾‘ s3_aksk.envï¼Œæ·»åŠ æ‚¨çš„å‡­è¯
```

### 4. åˆ›å»ºé…ç½®æ–‡ä»¶

å‚è€ƒ `examples/conf/` ç›®å½•ç»“æ„åˆ›å»ºé…ç½®ï¼š

```
conf/
â”œâ”€â”€ pipeline.toml           # ä¸»é…ç½®
â”œâ”€â”€ nodes/
â”‚   â””â”€â”€ node_1_v1.0.toml   # èŠ‚ç‚¹é…ç½®
â””â”€â”€ operators/
    â””â”€â”€ op_1_v1.0.toml     # ç®—å­é…ç½®
```

### 5. éªŒè¯é…ç½®

```bash
pipeline-cli validate --conf conf/pipeline.toml --config-root conf/
```

### 6. åˆå§‹åŒ–é¡¹ç›®

```bash
pipeline-cli init --conf conf/pipeline.toml --config-root conf/
```

### 7. è¿è¡Œäº§çº¿

```bash
# è¿è¡Œå…¨éƒ¨
pipeline-cli run --conf conf/pipeline.toml --skip-init

# è¿è¡Œå•ä¸ªèŠ‚ç‚¹
pipeline-cli run --conf conf/pipeline.toml --node node_1 --skip-init

# è¿è¡Œå•ä¸ªç®—å­
pipeline-cli run --conf conf/pipeline.toml --operator op_1 --skip-init
```

## ğŸ“ é…ç½®ç¤ºä¾‹

### Pipelineé…ç½®ï¼ˆpipeline.tomlï¼‰

```toml
[pipeline]
pipeline_id = "data_pipeline_v1"
name = "æ•°æ®å¤„ç†äº§çº¿"
work_dir = "./pipeline_workspace"

[pipeline.log_config]
level = "INFO"
rotation = "10 GB"
retention = 30

[pipeline.nodes]
refs = ["node_1:v1.0"]

[pipeline.operators]
refs = ["cleaner:v2.0"]
```

### Nodeé…ç½®ï¼ˆnodes/node_1_v1.0.tomlï¼‰

```toml
[node_1]
node_id = "node_1"
operator_ids = ["cleaner"]
runner_count = 1

[node_1.resource]
cpu_request = 2.0
cpu_limit = 4.0
memory_request = 8.0
memory_limit = 16.0
gpu_request = 0
```

### Operatoré…ç½®ï¼ˆoperators/cleaner_v2.0.tomlï¼‰

```toml
[cleaner]
operator_id = "cleaner"
git_repo = "https://github.com/example/cleaner.git"
git_tag = "v2.0.0"
upstream_dependencies = []
start_command = "python main.py"
timeout = 1800

[cleaner.extra_env_vars]
BATCH_SIZE = "100"

[cleaner.env_config]
env_name = "cleaner_env"
pyproject_path = "pyproject.toml"  # UVç¯å¢ƒ
```

## ğŸ› ï¸ ç®—å­å¼€å‘

ç®—å­éœ€è¦ç»§æ‰¿ `BasicRunner` åŸºç±»ï¼š

```python
from general_pipeline.core.basic_runner import BasicRunner
import os

class DataCleaner(BasicRunner):
    def run(self) -> int:
        # è·¯å¾„ç”±Pipelineæ³¨å…¥
        # self.input_root - è¾“å…¥æ•°æ®ç›®å½•
        # self.output_root - è¾“å‡ºæ•°æ®ç›®å½•  
        # self.workspace_root - å·¥ä½œç©ºé—´ç›®å½•
        
        # ç¯å¢ƒå˜é‡ç”±Pipelineæ³¨å…¥
        # PIPELINE_ID, NODE_ID, OPERATOR_ID
        
        # å®ç°æ•°æ®å¤„ç†é€»è¾‘
        data = self.load_data(self.input_root)
        cleaned = self.clean(data)
        self.save_data(cleaned, self.output_root)
        
        return 0  # è¿”å›exit code: 0=æˆåŠŸ

if __name__ == "__main__":
    cleaner = DataCleaner(
        pipeline_id=os.environ["PIPELINE_ID"],
        node_id=os.environ["NODE_ID"],
        operator_id=os.environ["OPERATOR_ID"],
        input_root=os.environ["INPUT_ROOT"],
        output_root=os.environ["OUTPUT_ROOT"],
        workspace_root=os.environ["WORKSPACE_ROOT"]
    )
    exit(cleaner.run())
```

## ğŸ³ Dockeréƒ¨ç½²

### Dockerfileç¤ºä¾‹

```dockerfile
# æ„å»ºé˜¶æ®µ - åˆå§‹åŒ–
FROM python:3.11 as builder
WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install -r requirements.txt

# å¤åˆ¶é…ç½®å’Œä»£ç 
COPY conf/ conf/
COPY s3_aksk.env .
RUN touch .project_root

# åˆå§‹åŒ–ï¼šå…‹éš†ä»£ç ã€åˆ›å»ºç¯å¢ƒ
RUN pipeline-cli init --conf conf/pipeline.toml --config-root conf/

# è¿è¡Œé˜¶æ®µ - ä»…æ‰§è¡Œ
FROM python:3.11
WORKDIR /app

# ä»æ„å»ºé˜¶æ®µå¤åˆ¶
COPY --from=builder /app /app

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼šé…ç½®è¦†ç›–ï¼‰
ENV PIPELINE_CONFIG_OVERRIDE_S3_PATH=""

# è¿è¡Œäº§çº¿
CMD ["pipeline-cli", "run", "--conf", "conf/pipeline.toml", "--skip-init"]
```

## ğŸ” S3å¯¹è±¡å­˜å‚¨

### å‡­è¯é…ç½®ï¼ˆs3_aksk.envï¼‰

```env
# ç«å±±å¼•æ“TOS
TOS_MY_BUCKET_ENDPOINT=https://tos-cn-beijing.volces.com
TOS_MY_BUCKET_ACCESS_KEY=your_access_key
TOS_MY_BUCKET_SECRET_KEY=your_secret_key
TOS_MY_BUCKET_REGION=cn-beijing

# AWS S3
S3_ANOTHER_BUCKET_ENDPOINT=https://s3.amazonaws.com
S3_ANOTHER_BUCKET_ACCESS_KEY=your_access_key
S3_ANOTHER_BUCKET_SECRET_KEY=your_secret_key
S3_ANOTHER_BUCKET_REGION=us-east-1
```

### ä½¿ç”¨S3

```python
from general_pipeline.utils.s3_utils import download_from_s3, upload_to_s3

# ä¸‹è½½åˆ°æœ¬åœ°
local_file = download_from_s3("tos://my-bucket/data/file.csv", "/tmp/file.csv")

# ä¸‹è½½åˆ°å†…å­˜
buffer = download_from_s3("tos://my-bucket/config/override.toml")
config = toml.loads(buffer.read().decode('utf-8'))

# ä¸Šä¼ æ–‡ä»¶
upload_to_s3("/tmp/output.csv", "tos://my-bucket/results/output.csv")
```

### Condaç¯å¢ƒä»S3

```toml
[operator.env_config]
env_name = "my_conda_env"
s3_compress_path = "tos://envs-bucket/conda/my_env.zst"
need_conda_update = true
```

## âš™ï¸ é«˜çº§åŠŸèƒ½

### é…ç½®è¦†ç›–

ä»S3åŠ¨æ€åŠ è½½é…ç½®è¦†ç›–ï¼š

```bash
export PIPELINE_CONFIG_OVERRIDE_S3_PATH="tos://config-bucket/prod-override.toml"
pipeline-cli run --conf conf/pipeline.toml
```

### é€‰æ‹©æ€§æ‰§è¡Œ

```bash
# åªè¿è¡Œç‰¹å®šç®—å­ï¼ˆæµ‹è¯•ï¼‰
pipeline-cli run --conf conf/pipeline.toml --operator data_cleaner

# åªè¿è¡Œç‰¹å®šèŠ‚ç‚¹ï¼ˆéƒ¨åˆ†æ‰§è¡Œï¼‰
pipeline-cli run --conf conf/pipeline.toml --node preprocessing_node

# è¿è¡Œå…¨éƒ¨ï¼ˆç”Ÿäº§ï¼‰
pipeline-cli run --conf conf/pipeline.toml
```

### GPUç›‘æ§

```python
from general_pipeline.core.resource_monitor import ResourceMonitor

# å¯ç”¨GPUç›‘æ§
monitor = ResourceMonitor(pid, monitor_gpu=True)
usage = monitor.get_resource_usage()
# è¿”å›: {"cpu_usage": 45.2, "mem_usage_mb": 1024, "gpu_0_util": 80, ...}
```

### Base64ç¼–ç æ•æ„Ÿä¿¡æ¯

```bash
# ç¼–ç 
pipeline-cli encode "my_secret_access_key"
# è¾“å‡ºï¼šbase64://bXlfc2VjcmV0X2FjY2Vzc19rZXk=

# åœ¨é…ç½®ä¸­ä½¿ç”¨
# ï¼ˆæ³¨æ„ï¼šç°åœ¨æ¨èä½¿ç”¨s3_aksk.envè€Œä¸æ˜¯åœ¨é…ç½®ä¸­å­˜å‚¨å‡­è¯ï¼‰
```

## ğŸ“‚ é¡¹ç›®ç»“æ„

```
general_pipeline/
â”œâ”€â”€ src/general_pipeline/
â”‚   â”œâ”€â”€ cli/                      # CLIå‘½ä»¤
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ basic_runner.py       # ç®—å­åŸºç±»
â”‚   â”‚   â”œâ”€â”€ pipeline_executor.py  # æ‰§è¡Œå™¨ï¼ˆrun/run_node/run_opï¼‰
â”‚   â”‚   â”œâ”€â”€ project_initiator.py  # åˆå§‹åŒ–å™¨
â”‚   â”‚   â””â”€â”€ resource_monitor.py   # èµ„æºç›‘æ§
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ env_config.py         # ç¯å¢ƒé…ç½®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ operator_config.py    # ç®—å­é…ç½®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ node_config.py        # èŠ‚ç‚¹é…ç½®æ¨¡å‹
â”‚   â”‚   â””â”€â”€ pipeline_config.py    # äº§çº¿é…ç½®æ¨¡å‹
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ codec.py              # Base64ç¼–è§£ç 
â”‚       â”œâ”€â”€ config_loader.py      # å±‚çº§åŒ–é…ç½®åŠ è½½å™¨
â”‚       â”œâ”€â”€ log_utils.py          # æ—¥å¿—å·¥å…·
â”‚       â”œâ”€â”€ path_utils.py         # è·¯å¾„å·¥å…·
â”‚       â”œâ”€â”€ s3_utils.py           # S3å·¥å…·ï¼ˆå«S3Pathæ¨¡å‹ï¼‰
â”‚       â”œâ”€â”€ subprocess_utils.py   # å­è¿›ç¨‹å·¥å…·
â”‚       â””â”€â”€ exceptions.py         # è‡ªå®šä¹‰å¼‚å¸¸
â”œâ”€â”€ examples/conf/                # ç¤ºä¾‹é…ç½®
â”‚   â”œâ”€â”€ pipeline.toml
â”‚   â”œâ”€â”€ nodes/
â”‚   â””â”€â”€ operators/
â”œâ”€â”€ doc/                          # æ¶æ„æ–‡æ¡£
â”œâ”€â”€ s3_aksk.env.example           # S3å‡­è¯æ¨¡æ¿
â””â”€â”€ pyproject.toml
```

## ğŸ“š æ–‡æ¡£

- [TOMLè¿ç§»æŒ‡å—](TOML_MIGRATION_GUIDE.md) - ä»YAMLè¿ç§»åˆ°TOML
- [å®ç°æ€»ç»“](REVIEW_ROUND_2_SUMMARY.md) - è¯¦ç»†å®ç°è¯´æ˜
- [æ¶æ„æ–‡æ¡£](doc/ai-instructions/) - è¯¦ç»†æ¶æ„è®¾è®¡

## ğŸ”„ é€€å‡ºç åè®®

- `0` - æˆåŠŸ
- `1` - é…ç½®é”™è¯¯
- `2` - è¾“å…¥é”™è¯¯
- `3` - æ‰§è¡Œé€»è¾‘é”™è¯¯
- `4` - èµ„æºå¼‚å¸¸ï¼ˆè¶…æ—¶ã€å†…å­˜ä¸è¶³ç­‰ï¼‰
- `5` - ç¯å¢ƒé”™è¯¯

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ™ è‡´è°¢

åŸºäº `doc/ai-instructions/` ä¸­çš„æ¶æ„è§„èŒƒæ„å»ºã€‚
